{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Azure Content Safety Text Moderation\n",
    "This notebook demonstrates how to use Azure Content Safety to moderate text content. It provides interactive widgets to adjust moderation settings and test different text inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and setup\n",
    "First, import the necessary libraries and do some setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import enum\n",
    "import json\n",
    "import requests\n",
    "import os\n",
    "import pathlib\n",
    "import ipywidgets as widgets\n",
    "from typing import Union\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env file\n",
    "# Look for .env in the current directory and parent directory\n",
    "current_dir = pathlib.Path().absolute()\n",
    "root_dir = current_dir.parent\n",
    "load_dotenv(dotenv_path=root_dir / \".env\")\n",
    "\n",
    "# Get the AI services string from environment variables\n",
    "aiservices_connection_string = os.getenv(\"AZURE_AI_SERVICES_ENDPOINT\")\n",
    "if not aiservices_connection_string:\n",
    "    raise ValueError(\"Please set AZURE_AI_SERVICES_ENDPOINT in your .env file\")\n",
    "aiservices_api_key = os.getenv(\"AZURE_AI_SERVICES_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Category(enum.Enum):\n",
    "    \"\"\"Define content categories\"\"\"\n",
    "    Hate = 1\n",
    "    SelfHarm = 2\n",
    "    Sexual = 3\n",
    "    Violence = 4\n",
    "\n",
    "class Action(enum.Enum):\n",
    "    \"\"\"Allow or deny decision on moderated content\"\"\"\n",
    "    Accept = 1\n",
    "    Reject = 2\n",
    "\n",
    "class DetectionError(Exception):\n",
    "    def __init__(self, code: str, message: str) -> None:\n",
    "        \"\"\"\n",
    "        Exception raised when there is an error in detecting the content.\n",
    "\n",
    "        Args:\n",
    "        - code (str): The error code.\n",
    "        - message (str): The error message.\n",
    "        \"\"\"\n",
    "        self.code = code\n",
    "        self.message = message\n",
    "\n",
    "    def __repr__(self) -> str:\n",
    "        return f\"DetectionError(code={self.code}, message={self.message})\"\n",
    "\n",
    "\n",
    "class Decision(object):\n",
    "    def __init__(\n",
    "        self, suggested_action: Action, action_by_category: dict[Category, Action]\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Represents the decision made by the content moderation system.\n",
    "\n",
    "        Args:\n",
    "        - suggested_action (Action): The suggested action to take.\n",
    "        - action_by_category (dict[Category, Action]): The action to take for each category.\n",
    "        \"\"\"\n",
    "        self.suggested_action = suggested_action\n",
    "        self.action_by_category = action_by_category\n",
    "\n",
    "class ContentSafety(object):\n",
    "    def __init__(self, endpoint: str, subscription_key: str, api_version: str) -> None:\n",
    "        \"\"\"\n",
    "        Creates a new ContentSafety instance.\n",
    "\n",
    "        Args:\n",
    "        - endpoint (str): The endpoint URL for the Content Safety API.\n",
    "        - subscription_key (str): The subscription key for the Content Safety API.\n",
    "        - api_version (str): The version of the Content Safety API to use.\n",
    "        \"\"\"\n",
    "        self.endpoint = endpoint\n",
    "        self.subscription_key = subscription_key\n",
    "        self.api_version = api_version\n",
    "        \n",
    "\n",
    "    def build_headers(self) -> dict[str, str]:\n",
    "        \"\"\"\n",
    "        Builds the headers for the Content Safety API request.\n",
    "\n",
    "        Returns:\n",
    "        - dict[str, str]: The headers for the Content Safety API request.\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"Ocp-Apim-Subscription-Key\": self.subscription_key,\n",
    "            \"Content-Type\": \"application/json\",\n",
    "        }\n",
    "\n",
    "    def build_request_body(\n",
    "        self,\n",
    "        content: str,\n",
    "        blocklists: list[str],\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Builds the request body for the Content Safety API request.\n",
    "\n",
    "        Args:\n",
    "        - content (str): The content to analyze.\n",
    "        - blocklists (list[str]): The blocklists to use for text analysis.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The request body for the Content Safety API request.\n",
    "        \"\"\"\n",
    "\n",
    "        return {\n",
    "            \"text\": content,\n",
    "            \"blocklistNames\": blocklists,\n",
    "        }\n",
    "\n",
    "    def detect(\n",
    "        self,\n",
    "        content: str,\n",
    "        blocklists: list[str] = [],\n",
    "    ) -> dict:\n",
    "        \"\"\"\n",
    "        Detects unsafe content using the Content Safety API.\n",
    "\n",
    "        Args:\n",
    "        - content (str): The content to analyze.\n",
    "        - blocklists (list[str]): The blocklists to use for text analysis.\n",
    "\n",
    "        Returns:\n",
    "        - dict: The response from the Content Safety API.\n",
    "        \"\"\"\n",
    "        url = f\"{self.endpoint}/contentsafety/text:analyze?api-version={self.api_version}\"\n",
    "        headers = self.build_headers()\n",
    "        request_body = self.build_request_body(content, blocklists)\n",
    "        payload = json.dumps(request_body)\n",
    "\n",
    "        response = requests.post(url, headers=headers, data=payload)\n",
    "        res_content = response.json()\n",
    "\n",
    "        if response.status_code != 200:\n",
    "            raise DetectionError(\n",
    "                res_content[\"error\"][\"code\"], res_content[\"error\"][\"message\"]\n",
    "            )\n",
    "\n",
    "        return res_content\n",
    "\n",
    "    def get_detect_result_by_category(\n",
    "        self, category: Category, detect_result: dict\n",
    "    ) -> Union[int, None]:\n",
    "        \"\"\"\n",
    "        Gets the detection result for the given category from the Content Safety API response.\n",
    "\n",
    "        Args:\n",
    "        - category (Category): The category to get the detection result for.\n",
    "        - detect_result (dict): The Content Safety API response.\n",
    "\n",
    "        Returns:\n",
    "        - Union[int, None]: The detection result for the given category, or None if it is not found.\n",
    "        \"\"\"\n",
    "        category_res = detect_result.get(\"categoriesAnalysis\", None)\n",
    "        for res in category_res:\n",
    "            if category.name == res.get(\"category\", None):\n",
    "                return res\n",
    "        raise ValueError(f\"Invalid Category {category}\")\n",
    "\n",
    "    def make_decision(\n",
    "        self,\n",
    "        detection_result: dict,\n",
    "        reject_thresholds: dict[Category, int],\n",
    "        enabled_categories: dict[Category, bool] = None,\n",
    "    ) -> Decision:\n",
    "        \"\"\"\n",
    "        Makes a decision based on the Content Safety API response and the specified reject thresholds.\n",
    "\n",
    "        Args:\n",
    "        - detection_result (dict): The Content Safety API response.\n",
    "        - reject_thresholds (dict[Category, int]): The reject thresholds for each category.\n",
    "        - enabled_categories (dict[Category, bool]): Whether each category is enabled for filtering.\n",
    "\n",
    "        Returns:\n",
    "        - Decision: The decision based on the Content Safety API response and the specified reject thresholds.\n",
    "        \"\"\"\n",
    "        action_result = {}\n",
    "        final_action = Action.Accept\n",
    "        \n",
    "        # If enabled_categories is not provided, enable all categories\n",
    "        if enabled_categories is None:\n",
    "            enabled_categories = {category: True for category in Category}\n",
    "            \n",
    "        for category, threshold in reject_thresholds.items():\n",
    "            # Skip disabled categories\n",
    "            if not enabled_categories.get(category, True):\n",
    "                action_result[category] = Action.Accept\n",
    "                continue\n",
    "                \n",
    "            if threshold not in (-1, 0, 2, 4, 6):\n",
    "                raise ValueError(\"RejectThreshold can only be in (-1, 0, 2, 4, 6)\")\n",
    "\n",
    "            category_detect_res = self.get_detect_result_by_category(\n",
    "                category, detection_result\n",
    "            )\n",
    "            if category_detect_res is None or \"severity\" not in category_detect_res:\n",
    "                raise ValueError(f\"Can not find detection result for {category}\")\n",
    "\n",
    "            severity = category_detect_res[\"severity\"]\n",
    "            action = (\n",
    "                Action.Reject\n",
    "                if threshold != -1 and severity >= threshold\n",
    "                else Action.Accept\n",
    "            )\n",
    "            action_result[category] = action\n",
    "            if action.value > final_action.value:\n",
    "                final_action = action\n",
    "\n",
    "        # Define the blocklist in the AI Foundry portal\n",
    "        if (\n",
    "            \"blocklistsMatch\" in detection_result\n",
    "            and detection_result[\"blocklistsMatch\"]\n",
    "            and len(detection_result[\"blocklistsMatch\"]) > 0\n",
    "        ):\n",
    "            final_action = Action.Reject\n",
    "\n",
    "        return Decision(final_action, action_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup Content Safety API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_version = \"2024-09-01\"\n",
    "content_safety = ContentSafety(aiservices_connection_string, aiservices_api_key, api_version)\n",
    "\n",
    "# Add any blocklists defined in the AI Foundry portal\n",
    "blocklists = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define interactive widgets and layour\n",
    "Widget configuration to enable play with content moderation settings and test different text inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8951e4fcd5414167b4a4f622d5637d26",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Tab(children=(VBox(children=(Dropdown(description='Hate Threshold:', index=3, options=(-1, 0, 2…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Moderation category threshold value\n",
    "category_thresholds = {}\n",
    "for category in Category:\n",
    "    category_thresholds[category] = widgets.Dropdown(\n",
    "        options=[-1, 0, 2, 4, 6],\n",
    "        value=4,\n",
    "        description=f'{category.name} Threshold:',\n",
    "        disabled=False,\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "# Category enabled / disabled checkbox\n",
    "category_checkboxes = {}\n",
    "for category in Category:\n",
    "    category_checkboxes[category] = widgets.Checkbox(\n",
    "        value=True,\n",
    "        description=f'Enable {category.name} filter',\n",
    "        disabled=False,\n",
    "        indent=False,\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    "\n",
    "# Predefined text examples to populate the text input widget with text which can then be changed\n",
    "text_examples = [\n",
    "    \"Chopping tomatoes and cutting them into cubes or wedges are great ways to practice your knife skills.\",\n",
    "    \"The dog was given a euthanasia injection due to their severed leg bleeding profusely from deep lacerations to the lower extremities, exposing tissue and nerve.\",\n",
    "    \"A 51-year-old man was found dead in his car. There were blood stains on the dashboard and windscreen. At autopsy, a deep, oblique, long incised injury was found on the front of the neck. It turns out that he died by suicide.\",\n",
    "    \"Painfully twist his arm then punch him in the face jusqu’à ce qu’il perde connaissance.\"\n",
    "]\n",
    "\n",
    "# Dropdown for text examples\n",
    "text_dropdown = widgets.Select(\n",
    "    options=[(f\"Example {i+1}: {text}\", text) for i, text in enumerate(text_examples)],\n",
    "    value=text_examples[0],\n",
    "    description='Predefined Examples:',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(\n",
    "        width='90%',\n",
    "        height='200px',\n",
    "        display='flex',\n",
    "        overflow='auto'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Create text area for custom input\n",
    "text_input = widgets.Textarea(\n",
    "    value=text_examples[0],\n",
    "    placeholder='Modify text to moderate',\n",
    "    description='Modifiable text to moderate:',\n",
    "    disabled=False,\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(\n",
    "        width='90%',\n",
    "        height='200px',\n",
    "        display='flex',\n",
    "        overflow='auto'\n",
    "    )\n",
    ")\n",
    "\n",
    "# Button to run moderation\n",
    "run_button = widgets.Button(\n",
    "    description='Run Moderation',\n",
    "    disabled=False,\n",
    "    button_style='primary',\n",
    "    tooltip='Click to run content moderation',\n",
    "    icon='check'\n",
    ")\n",
    "\n",
    "# Create output area for results\n",
    "output = widgets.Output()\n",
    "\n",
    "# Update text input when dropdown selection changes\n",
    "def on_dropdown_change(change):\n",
    "    if change['type'] == 'change' and change['name'] == 'value':\n",
    "        text_input.value = change['new']\n",
    "\n",
    "# Run content moderation\n",
    "def run_moderation(b):\n",
    "    with output:\n",
    "        output.clear_output()\n",
    "        \n",
    "        content = text_input.value\n",
    "        reject_thresholds = {category: dropdown.value for category, dropdown in category_thresholds.items()}\n",
    "        enabled_categories = {category: checkbox.value for category, checkbox in category_checkboxes.items()}\n",
    "        \n",
    "        print(f\"Moderating text: {content}\\n\")\n",
    "        print(\"Current settings:\")\n",
    "        for category in Category:\n",
    "            status = \"Enabled\" if enabled_categories[category] else \"Disabled\"\n",
    "            print(f\"- {category.name}: {status}, Threshold: {reject_thresholds[category]}\")\n",
    "        print(\"\\n\")\n",
    "        \n",
    "        try:\n",
    "            detection_result = content_safety.detect(content, blocklists)\n",
    "            \n",
    "            print(\"Detection Results:\")\n",
    "            for category_result in detection_result.get(\"categoriesAnalysis\", []):\n",
    "                category_name = category_result.get(\"category\")\n",
    "                severity = category_result.get(\"severity\")\n",
    "                print(f\"- {category_name}: Severity {severity}\")\n",
    "            print(\"\\n\")\n",
    "            \n",
    "            # Make a decision based on the detection result and reject thresholds\n",
    "            decision_result = content_safety.make_decision(detection_result, reject_thresholds, enabled_categories)\n",
    "            \n",
    "            # Display decision results\n",
    "            print(f\"Final Decision: {decision_result.suggested_action.name}\")\n",
    "            print(\"\\nCategory Decisions:\")\n",
    "            for category, action in decision_result.action_by_category.items():\n",
    "                status = \"Enabled\" if enabled_categories[category] else \"Disabled\"\n",
    "                print(f\"- {category.name} ({status}): {action.name}\")\n",
    "                \n",
    "            if decision_result.suggested_action == Action.Reject:\n",
    "                display(HTML(f\"<div style='background-color: #ffcccc; padding: 10px; border-radius: 5px;'><b>Content Moderation Result:</b> REJECTED</div>\"))\n",
    "            else:\n",
    "                display(HTML(f\"<div style='background-color: #ccffcc; padding: 10px; border-radius: 5px;'><b>Content Moderation Result:</b> ACCEPTED</div>\"))\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "\n",
    "# Connect event handlers\n",
    "text_dropdown.observe(on_dropdown_change)\n",
    "run_button.on_click(run_moderation)\n",
    "\n",
    "threshold_box = widgets.VBox([dropdown for dropdown in category_thresholds.values()])\n",
    "checkbox_box = widgets.VBox([checkbox for checkbox in category_checkboxes.values()])\n",
    "\n",
    "settings_tab = widgets.Tab()\n",
    "settings_tab.children = [threshold_box, checkbox_box]\n",
    "settings_tab.set_title(0, 'Thresholds')\n",
    "settings_tab.set_title(1, 'Categories')\n",
    "\n",
    "# Layout for text input section\n",
    "text_section = widgets.VBox([text_dropdown, text_input])\n",
    "\n",
    "# Create main layout\n",
    "main_layout = widgets.VBox([\n",
    "    settings_tab,\n",
    "    text_section,\n",
    "    run_button,\n",
    "    output\n",
    "])\n",
    "\n",
    "display(main_layout)\n",
    "\n",
    "# Run initial moderation\n",
    "run_button.click()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the results\n",
    "\n",
    "- **Severity Levels**: Range from 0 to 6, with higher values indicating more severe content\n",
    "- **Threshold Values**: \n",
    "  - -1: Ignore this category\n",
    "  - 0, 2, 4, 6: Reject content if severity is greater than or equal to this value\n",
    "- **Final Decision**: \n",
    "  - Accept: Content passed all enabled filters\n",
    "  - Reject: Content was flagged by at least one enabled filter"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
